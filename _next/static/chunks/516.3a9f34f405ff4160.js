(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[516],{5789:function(){},6516:function(e,t,i){"use strict";var n;function o(e){return void 0===e||e}function a(e){let t=Array(e);for(let i=0;i<e;i++)t[i]=s();return t}function s(){return Object.create(null)}function r(e,t){return t.length-e.length}function l(e){return"string"==typeof e}function h(e){return"object"==typeof e}function c(e){return"function"==typeof e}function d(e,t){var i=u;if(e&&(t&&(e=m(e,t)),this.H&&(e=m(e,this.H)),this.J&&1<e.length&&(e=m(e,this.J)),i||""===i)){if(t=e.split(i),this.filter){e=this.filter,i=t.length;let n=[];for(let o=0,a=0;o<i;o++){let i=t[o];i&&!e[i]&&(n[a++]=i)}e=n}else e=t}return e}i.r(t),i.d(t,{search:function(){return $}});let u=/[\p{Z}\p{S}\p{P}\p{C}]+/u,f=/[\u0300-\u036f]/g;function p(e,t){let i=Object.keys(e),n=i.length,o=[],a="",s=0;for(let r=0,l,h;r<n;r++)(h=e[l=i[r]])?(o[s++]=g(t?"(?!\\b)"+l+"(\\b|_)":l),o[s++]=h):a+=(a?"|":"")+l;return a&&(o[s++]=g(t?"(?!\\b)("+a+")(\\b|_)":"("+a+")"),o[s]=""),o}function m(e,t){for(let i=0,n=t.length;i<n&&(e=e.replace(t[i],t[i+1]));i+=2);return e}function g(e){return RegExp(e,"g")}function b(e){let t="",i="";for(let n=0,o=e.length,a;n<o;n++)(a=e[n])!==i&&(t+=i=a);return t}function w(e){return d.call(this,(""+e).toLowerCase(),!1)}let y={},v={};function x(e){k(e,"add"),k(e,"append"),k(e,"search"),k(e,"update"),k(e,"remove")}function k(e,t){e[t+"Async"]=function(){let e;let i=this,n=arguments;var o=n[n.length-1];return c(o)&&(e=o,delete n[n.length-1]),o=new Promise(function(e){setTimeout(function(){i.async=!0;let o=i[t].apply(i,n);i.async=!1,e(o)})}),e?(o.then(e),this):o}}function T(e,t,i,n){let o=e.length,a=[],r,l,h=0;n&&(n=[]);for(let c=o-1;0<=c;c--){let d=e[c],u=d.length,f=s(),p=!r;for(let e=0;e<u;e++){let s=d[e],u=s.length;if(u)for(let e=0,d,m;e<u;e++)if(m=s[e],r){if(r[m]){if(!c){if(i)i--;else if(a[h++]=m,h===t)return a}(c||n)&&(f[m]=1),p=!0}if(n&&(d=(l[m]||0)+1,l[m]=d,d<o)){let e=n[d-2]||(n[d-2]=[]);e[e.length]=m}}else f[m]=1}if(n)r||(l=f);else if(!p)return[];r=f}if(n)for(let e=n.length-1,o,s;0<=e;e--){s=(o=n[e]).length;for(let e=0,n;e<s;e++)if(!r[n=o[e]]){if(i)i--;else if(a[h++]=n,h===t)return a;r[n]=1}}return a}function A(e){this.l=!0!==e&&e,this.cache=s(),this.h=[]}function I(e,t,i){h(e)&&(e=e.query);let n=this.cache.get(e);return n||(n=this.search(e,t,i),this.cache.set(e,n)),n}A.prototype.set=function(e,t){if(!this.cache[e]){var i=this.h.length;for(i===this.l?delete this.cache[this.h[i-1]]:i++,--i;0<i;i--)this.h[i]=this.h[i-1];this.h[0]=e}this.cache[e]=t},A.prototype.get=function(e){let t=this.cache[e];if(this.l&&t&&(e=this.h.indexOf(e))){let t=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=t}return t};let P={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function L(e,t,i,n,o,a,s,r){setTimeout(function(){let l=e(i?i+"."+n:n,JSON.stringify(s));l&&l.then?l.then(function(){t.export(e,t,i,o,a+1,r)}):t.export(e,t,i,o,a+1,r)})}function G(e,t){if(!(this instanceof G))return new G(e);if(e){l(e)?e=P[e]:(i=e.preset)&&(e=Object.assign({},i[i],e)),i=e.charset;var i,n=e.lang;l(i)&&(-1===i.indexOf(":")&&(i+=":default"),i=v[i]),l(n)&&(n=y[n])}else e={};let r,h,c=e.context||{};if(this.encode=e.encode||i&&i.encode||w,this.register=t||s(),this.D=r=e.resolution||9,this.G=t=i&&i.G||e.tokenize||"strict",this.depth="strict"===t&&c.depth,this.l=o(c.bidirectional),this.s=h=o(e.optimize),this.m=o(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=h?a(r):s(),this.A=r=c.resolution||1,this.h=h?a(r):s(),this.F=i&&i.F||e.rtl,this.H=(t=e.matcher||n&&n.H)&&p(t,!1),this.J=(t=e.stemmer||n&&n.J)&&p(t,!0),i=t=e.filter||n&&n.filter){i=t,n=s();for(let e=0,t=i.length;e<t;e++)n[i[e]]=1;i=n}this.filter=i,this.cache=(t=e.cache)&&new A(t)}function O(e,t,i,n,o){return i&&1<e?t+(n||0)<=e?i+(o||0):(e-1)/(t+(n||0))*(i+(o||0))+1|0:0}function S(e,t,i,n,o,a,r){let l=r?e.h:e.map;(!t[i]||r&&!t[i][r])&&(e.s&&(l=l[n]),r?((t=t[i]||(t[i]=s()))[r]=1,l=l[r]||(l[r]=s())):t[i]=1,l=l[i]||(l[i]=[]),e.s||(l=l[n]||(l[n]=[])),a&&l.includes(o)||(l[l.length]=o,e.m&&((e=e.register[o]||(e.register[o]=[]))[e.length]=l)))}function F(e,t,i,n,o,a,s,r){let l=[],h=r?e.h:e.map;if(e.s||(h=C(h,s,r,e.l)),h){let i=0,c=Math.min(h.length,r?e.A:e.D);for(let t=0,d=0,u,f;t<c&&(!(u=h[t])||(e.s&&(u=C(u,s,r,e.l)),o&&u&&a&&((f=u.length)<=o?(o-=f,u=null):(u=u.slice(o),o=0)),!u||(l[i++]=u,!a||!((d+=u.length)>=n))));t++);if(i)return a?z(l,n,0):void(t[t.length]=l)}return!i&&l}function z(e,t,i){return e=1===e.length?e[0]:[].concat.apply([],e),i||e.length>t?e.slice(i,i+t):e}function C(e,t,i,n){return e=i?(e=e[(n=n&&t>i)?t:i])&&e[n?i:t]:e[t]}function q(e,t,i,n,o){let a=0;if(e.constructor===Array){if(o)-1!==(t=e.indexOf(t))?1<e.length&&(e.splice(t,1),a++):a++;else{o=Math.min(e.length,i);for(let s=0,r;s<o;s++)(r=e[s])&&(a=q(r,t,i,n,o),n||a||delete e[s])}}else for(let s in e)(a=q(e[s],t,i,n,o))||delete e[s];return a}function R(e){e=e.data;var t=self._index;let i=e.args;var n=e.task;"init"===n?(n=e.options||{},e=e.factory,t=n.encode,n.cache=!1,t&&0===t.indexOf("function")&&(n.encode=Function("return "+t)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(n),delete self.FlexSearch):self._index=new G(n)):(e=e.id,t=t[n].apply(t,i),postMessage("search"===n?{id:e,msg:t}:{id:e}))}(n=G.prototype).append=function(e,t){return this.add(e,t,!0)},n.add=function(e,t,i,n){if(t&&(e||0===e)){if(!n&&!i&&this.register[e])return this.update(e,t);if(n=(t=this.encode(t)).length){let c=s(),d=s(),u=this.depth,f=this.D;for(let p=0;p<n;p++){let m=t[this.F?n-1-p:p];var o=m.length;if(m&&o>=this.B&&(u||!d[m])){var a=O(f,n,p),r="";switch(this.G){case"full":if(2<o){for(a=0;a<o;a++)for(var l=o;l>a;l--)if(l-a>=this.B){var h=O(f,n,p,o,a);S(this,d,r=m.substring(a,l),h,e,i)}break}case"reverse":if(1<o){for(l=o-1;0<l;l--)(r=m[l]+r).length>=this.B&&S(this,d,r,O(f,n,p,o,l),e,i);r=""}case"forward":if(1<o){for(l=0;l<o;l++)(r+=m[l]).length>=this.B&&S(this,d,r,a,e,i);break}default:if(this.C&&(a=Math.min(a/this.C(t,m,p)|0,f-1)),S(this,d,m,a,e,i),u&&1<n&&p<n-1){for(o=s(),r=this.A,a=m,l=Math.min(u+1,n-p),o[a]=1,h=1;h<l;h++)if((m=t[this.F?n-1-p-h:p+h])&&m.length>=this.B&&!o[m]){o[m]=1;let t=this.l&&m>a;S(this,c,t?a:m,O(r+(n/2>r?0:1),n,p,l-1,h-1),e,i,t?m:a)}}}}}this.m||(this.register[e]=1)}}return this},n.search=function(e,t,i){let n,o,a;i||(!t&&h(e)?e=(i=e).query:h(t)&&(i=t));let l=[],c,d,u=0;if(i){e=i.query||e,t=i.limit,u=i.offset||0;var f=i.context;d=i.suggest}if(e&&1<(c=(e=this.encode(""+e)).length)){i=s();var p=[];for(let t=0,n=0,o;t<c;t++)if((o=e[t])&&o.length>=this.B&&!i[o]){if(!this.s&&!d&&!this.map[o])return l;p[n++]=o,i[o]=1}c=(e=p).length}if(!c)return l;for(t||(t=100),f=this.depth&&1<c&&!1!==f,i=0,f?(n=e[0],i=1):1<c&&e.sort(r);i<c;i++){if(a=e[i],f?(o=F(this,l,d,t,u,2===c,a,n),d&&!1===o&&l.length||(n=a)):o=F(this,l,d,t,u,1===c,a),o)return o;if(d&&i===c-1){if(!(p=l.length)){if(f){f=0,i=-1;continue}return l}if(1===p)return z(l[0],t,u)}}return T(l,t,u,d)},n.contain=function(e){return!!this.register[e]},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e,t){let i=this.register[e];if(i){if(this.m)for(let t=0,n;t<i.length;t++)(n=i[t]).splice(n.indexOf(e),1);else q(this.map,e,this.D,this.s),this.depth&&q(this.h,e,this.A,this.s);if(t||delete this.register[e],this.cache){t=this.cache;for(let i=0,n;i<t.h.length;i++)n=t.h[i],t.cache[n].includes(e)&&(t.h.splice(i--,1),delete t.cache[n])}}return this},n.searchCache=I,n.export=function(e,t,i,n,o,a){let r,l,h=!0;switch(void 0===a&&(h=new Promise(e=>{a=e})),o||(o=0)){case 0:if(r="reg",this.m)for(let e in l=s(),this.register)l[e]=1;else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:void 0===i&&a&&a();return}return L(e,t||this,i,r,n,o,l,a),h},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"cfg":this.s=!!t.opt;break;case"reg":this.m=!1,this.register=t;break;case"map":this.map=t;break;case"ctx":this.h=t}},x(G.prototype);let M=0;function D(e){var t;if(!(this instanceof D))return new D(e);e?c(t=e.encode)&&(e.encode=t.toString()):e={},(t=(self||window)._factory)&&(t=t.toString());let n="undefined"==typeof window&&self.exports,o=this;this.o=function(e,t,n){let o;try{o=t?new(i(5789)).Worker("//node/node.js"):e?new Worker(URL.createObjectURL(new Blob(["onmessage="+R.toString()],{type:"text/javascript"}))):new Worker(l(n)?n:"worker/worker.js",{type:"module"})}catch(e){}return o}(t,n,e.worker),this.h=s(),this.o&&(n?this.o.on("message",function(e){o.h[e.id](e.msg),delete o.h[e.id]}):this.o.onmessage=function(e){e=e.data,o.h[e.id](e.msg),delete o.h[e.id]},this.o.postMessage({task:"init",factory:t,options:e}))}function j(e){D.prototype[e]=D.prototype[e+"Async"]=function(){let t;let i=this,n=[].slice.call(arguments);var o=n[n.length-1];return c(o)&&(t=o,n.splice(n.length-1,1)),o=new Promise(function(t){setTimeout(function(){i.h[++M]=t,i.o.postMessage({task:e,id:M,args:n})})}),t?(o.then(t),this):o}}function B(e){if(!(this instanceof B))return new B(e);var t,i=e.document||e.doc||e;this.K=[],this.h=[],this.A=[],this.register=s(),this.key=(t=i.key||i.id)&&H(t,this.A)||"id",this.m=o(e.fastupdate),this.C=(t=i.store)&&!0!==t&&[],this.store=t&&s(),this.I=(t=i.tag)&&H(t,this.A),this.l=t&&s(),this.cache=(t=e.cache)&&new A(t),e.cache=!1,this.o=e.worker,this.async=!1,t=s();let n=i.index||i.field||i;l(n)&&(n=[n]);for(let i=0,o,a;i<n.length;i++)l(o=n[i])||(a=o,o=o.field),a=h(a)?Object.assign({},e,a):e,this.o&&(t[o]=new D(a),t[o].o||(this.o=!1)),this.o||(t[o]=new G(a,this.register)),this.K[i]=H(o,this.A),this.h[i]=o;if(this.C)for(l(e=i.store)&&(e=[e]),i=0;i<e.length;i++)this.C[i]=H(e[i],this.A);this.index=t}function H(e,t){let i=e.split(":"),n=0;for(let o=0;o<i.length;o++)0<=(e=i[o]).indexOf("[]")&&(e=e.substring(0,e.length-2))&&(t[n]=!0),e&&(i[n++]=e);return n<i.length&&(i.length=n),1<n?i:i[0]}function E(e,t){if(l(t))e=e[t];else for(let i=0;e&&i<t.length;i++)e=e[t[i]];return e}function W(e,t,i,n){let o=this.l[e],a=o&&o.length-i;if(a&&0<a)return(a>t||i)&&(o=o.slice(i,i+t)),n&&(o=U.call(this,o)),{tag:e,result:o}}function U(e){let t=Array(e.length);for(let i=0,n;i<e.length;i++)n=e[i],t[i]={id:n,doc:this.store[n]};return t}j("add"),j("append"),j("search"),j("update"),j("remove"),(n=B.prototype).add=function(e,t,i){if(h(e)&&(e=E(t=e,this.key)),t&&(e||0===e)){if(!i&&this.register[e])return this.update(e,t);for(let n=0,o,a;n<this.h.length;n++)a=this.h[n],l(o=this.K[n])&&(o=[o]),function e(t,i,n,o,a,s,r,l){if(t=t[r]){if(o===i.length-1){if(t.constructor===Array){if(n[o]){for(i=0;i<t.length;i++)a.add(s,t[i],!0,!0);return}t=t.join(" ")}a.add(s,t,l,!0)}else if(t.constructor===Array)for(r=0;r<t.length;r++)e(t,i,n,o,a,s,r,l);else r=i[++o],e(t,i,n,o,a,s,r,l)}}(t,o,this.A,0,this.index[a],e,o[0],i);if(this.I){let n=E(t,this.I),o=s();l(n)&&(n=[n]);for(let t=0,a,s;t<n.length;t++)if(!o[a=n[t]]&&(o[a]=1,s=this.l[a]||(this.l[a]=[]),!i||!s.includes(e))&&(s[s.length]=e,this.m)){let t=this.register[e]||(this.register[e]=[]);t[t.length]=s}}if(this.store&&(!i||!this.store[e])){let i;if(this.C){i=s();for(let e=0,n;e<this.C.length;e++)l(n=this.C[e])?i[n]=t[n]:function e(t,i,n,o,a){if(t=t[a],o===n.length-1)i[a]=t;else if(t){if(t.constructor===Array)for(i=i[a]=Array(t.length),a=0;a<t.length;a++)e(t,i,n,o,a);else i=i[a]||(i[a]=s()),a=n[++o],e(t,i,n,o,a)}}(t,i,n,0,n[0])}this.store[e]=i||t}}return this},n.append=function(e,t){return this.add(e,t,!0)},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e){if(h(e)&&(e=E(e,this.key)),this.register[e]){for(var t=0;t<this.h.length&&(this.index[this.h[t]].remove(e,!this.o),!this.m);t++);if(this.I&&!this.m)for(let i in this.l){let n=(t=this.l[i]).indexOf(e);-1!==n&&(1<t.length?t.splice(n,1):delete this.l[i])}this.store&&delete this.store[e],delete this.register[e]}return this},n.search=function(e,t,i,n){i||(!t&&h(e)?(i=e,e=""):h(t)&&(i=t,t=0));let o=[],a=[],r,c,d,u,f,p,m=0;if(i){if(i.constructor===Array)d=i,i=null;else{if(e=i.query||e,d=(r=i.pluck)||i.index||i.field,u=i.tag,c=this.store&&i.enrich,f="and"===i.bool,t=i.limit||t||100,p=i.offset||0,u&&(l(u)&&(u=[u]),!e)){for(let e=0,i;e<u.length;e++)(i=W.call(this,u[e],t,p,c))&&(o[o.length]=i,m++);return m?o:[]}l(d)&&(d=[d])}}d||(d=this.h),f=f&&(1<d.length||u&&1<u.length);let g=!n&&(this.o||this.async)&&[];for(let r=0,h,b,w;r<d.length;r++){let y;if(l(b=d[r])||(b=(y=b).field,e=y.query||e,t=y.limit||t,c=y.enrich||c),g)g[r]=this.index[b].searchAsync(e,t,y||i);else{if(w=(h=n?n[r]:this.index[b].search(e,t,y||i))&&h.length,u&&w){let e=[],i=0;f&&(e[0]=[h]);for(let t=0,n,o;t<u.length;t++)n=u[t],(w=(o=this.l[n])&&o.length)&&(i++,e[e.length]=f?[o]:o);i&&(w=(h=f?T(e,t||100,p||0):function(e,t){let i=s(),n=s(),o=[];for(let t=0;t<e.length;t++)i[e[t]]=1;for(let e=0,a;e<t.length;e++){a=t[e];for(let e=0,t;e<a.length;e++)i[t=a[e]]&&!n[t]&&(n[t]=1,o[o.length]=t)}return o}(h,e)).length)}if(w)a[m]=b,o[m++]=h;else if(f)return[]}}if(g){let n=this;return new Promise(function(o){Promise.all(g).then(function(a){o(n.search(e,t,i,a))})})}if(!m)return[];if(r&&(!c||!this.store))return o[0];for(let e=0,t;e<a.length;e++){if((t=o[e]).length&&c&&(t=U.call(this,t)),r)return t;o[e]={field:a[e],result:t}}return o},n.contain=function(e){return!!this.register[e]},n.get=function(e){return this.store[e]},n.set=function(e,t){return this.store[e]=t,this},n.searchCache=I,n.export=function(e,t,i,n,o,a){let s;if(void 0===a&&(s=new Promise(e=>{a=e})),o||(o=0),n||(n=0),n<this.h.length){let i=this.h[n],s=this.index[i];t=this,setTimeout(function(){s.export(e,t,o?i:"",n,o++,a)||(n++,o=1,t.export(e,t,i,n,o,a))})}else{let t,s;switch(o){case 1:t="tag",s=this.l,i=null;break;case 2:t="store",s=this.store,i=null;break;default:a();return}L(e,this,i,t,n,o,s,a)}return s},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"tag":this.l=t;break;case"reg":this.m=!1,this.register=t;for(let e=0,i;e<this.h.length;e++)(i=this.index[this.h[e]]).register=t,i.m=!1;break;case"store":this.store=t;break;default:let i=(e=e.split("."))[0];e=e[1],i&&e&&this.index[i].import(e,t)}},x(B.prototype);let N=[g("[\xe0\xe1\xe2\xe3\xe4\xe5]"),"a",g("[\xe8\xe9\xea\xeb]"),"e",g("[\xec\xed\xee\xef]"),"i",g("[\xf2\xf3\xf4\xf5\xf6ő]"),"o",g("[\xf9\xfa\xfb\xfcű]"),"u",g("[\xfdŷ\xff]"),"y",g("\xf1"),"n",g("[\xe7c]"),"k",g("\xdf"),"s",g(" & ")," and "];function V(e){var t=e=""+e;return t.normalize&&(t=t.normalize("NFD").replace(f,"")),d.call(this,t.toLowerCase(),!e.normalize&&N)}let _=/[^a-z0-9]+/,K={b:"p",v:"f",w:"f",z:"s",x:"s",ß:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Z(e){e=V.call(this,e).join(" ");let t=[];if(e){let i=e.split(_),n=i.length;for(let o=0,a,s=0;o<n;o++)if((e=i[o])&&(!this.filter||!this.filter[e])){let i=K[a=e[0]]||a,n=i;for(let t=1;t<e.length;t++){let o=K[a=e[t]]||a;o&&o!==n&&(i+=o,n=o)}t[s++]=i}}return t}let J=[g("ae"),"a",g("oe"),"o",g("sh"),"s",g("th"),"t",g("ph"),"f",g("pf"),"f",g("(?![aeo])h(?![aeo])"),"",g("(?!^[aeo])h(?!^[aeo])"),""];function Y(e,t){return e&&(2<(e=Z.call(this,e).join(" ")).length&&(e=m(e,J)),t||(1<e.length&&(e=b(e)),e&&(e=e.split(" ")))),e||[]}let Q=g("(?!\\b)[aeo]");v["latin:default"]={encode:w,F:!1,G:""},v["latin:simple"]={encode:V,F:!1,G:""},v["latin:balance"]={encode:Z,F:!1,G:"strict"},v["latin:advanced"]={encode:Y,F:!1,G:""},v["latin:extra"]={encode:function(e){return e&&(1<(e=Y.call(this,e,!0)).length&&(e=e.replace(Q,"")),1<e.length&&(e=b(e)),e&&(e=e.split(" "))),e||[]},F:!1,G:""};let X=new({Index:G,Document:B,Worker:D,registerCharset:function(e,t){v[e]=t},registerLanguage:function(e,t){y[e]=t}}).Document({tokenize:"full",document:{id:"url",index:"content",store:["title","pageTitle"]},context:{resolution:9,depth:2,bidirectional:!0}});for(let{url:e,sections:t}of[{url:"/",sections:[["Quick start",null,["Scikit-LLM allows you to seamlessly integrate powerful language models into scikit-learn for enhanced text analysis tasks.","Let's see how it is possible to use Scikit-LLM to perform zero-shot text classification with GPT-4."]],["Installation","installation",["First of all, it is necessary to install Scikit-LLM. You can do it by running the following command:"]],["API Key Configuration","api-key-configuration",["For this example, we will use GPT-4, which requires an OpenAI API key. You can get one here.","Once you have your API key, you can set it as follows:","Scikit-LLM supports other language models, including the locally hosted ones. For more information, please refer to the Backend Families section."]],["Zero-Shot Text Classification","zero-shot-text-classification",["Now, we are ready to perform zero-shot text classification with GPT-4. Let's start by loading a sample dataset:","Then, we can create a classifier instance and fit it using conventional scikit-learn syntax:","Scikit-LLM will automatically query the OpenAI API and transform the response into a regular list of labels.",'Additionally, Scikit-LLM will ensure that the obtained response contains a valid label. If this is not the case, a label will be selected randomly (label probabilities are proportional to label occurrences in the "training" set).','Furthermore, since the "training" data is not strictly required, it can be fully omitted. The only thing that has to be provided is the list of candidate labels.']]]},{url:"/docs/dynamic-few-shot-text-classification",sections:[["Dynamic few-shot text classification",null,[]],["Overview","overview",["Dynamic Few-Shot Classification is an extension of Few-Shot Text Classification that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens.","Let's consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:","Now let's say we want to classify the following review:","Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:","This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","DynamicFewShotGPTClassifier"]]]},{url:"/docs/few-shot-text-classification",sections:[["Few-shot text classification",null,[]],["Overview","overview",["Few-shot text classification is a task of classifying a text into one of the pre-defined classes based on a few examples of each class. For example, given a few examples of the class positive, negative, and neutral, the model should be able to classify a new text into one of these classes.","The estimators provided by Scikit-LLM do not automatically select the subset of the training data, and instead use the entire training set to construct the examples. Therefore, if your training set is large, you might want to consider splitting it into training and validation sets, while keeping the training set small (we recommend not to exceed 10 examples per class).","Example using GPT-4:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","FewShotGPTClassifier","MultiLabelFewShotGPTClassifier"]]]},{url:"/docs/how-to-contribute",sections:[["How to contribute \uD83E\uDD17",null,["Welcome! We appreciate your interest in contributing to . Whether you're a developer, designer, writer, or simply passionate about open source, there are several ways you can help improve this project. This document will guide you through the process of contributing to our Python repository."]],["How Can I Contribute?","how-can-i-contribute",["There are several ways you can contribute to this project:","Before contributing, we recommend that you open an issue to discuss your planned changes. This allows us to align our goals, provide guidance, and potentially find other contributors interested in collaborating on the same feature or bug fix.","When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license."]]]},{url:"/docs/introduction-backend-families",sections:[["Backend families",null,["On a high level, Scikit-LLM estimators are divided based on the language model backend family they use. The backend family is defined by the API format and does not necessarily correspond to the language model architecture. For example, all backends that follow the OpenAI API format are groupped into gpt family regardless the actual language model architecture or provider. Eeach backend family has its own set of estimators which are located in the  sub-module.","For example, the Zero-Shot Classifier is available as  for the gpt family, and as  for the vertex family. The separation between the backend families is necessary to allow for a reasonable level of flexibility if/when model providers introduce model-specific features that are not supported by other providers and hence cannot be easily abstracted away. At the same time, the number of model families is kept to a minimum to simplify the usage and maintenance of the library. Since the OpenAI API is by far the most popular and widely used, backends that follow that format are preferred over the others.","Whenever the backend family supports multiple backends, the default one is used unless the  parameter specifies a particular backend namespace. For example, the default backend for the gpt family is the OpenAI backend. However, you can use the Azure backend by setting . However, please note that not every estimator supports every backend."]],["GPT Family","gpt-family",["The GPT family includes all backends that follow the OpenAI API format.","OpenAI (default)","The OpenAI backend is the default backend for the GPT family. It is used whenever the  parameter does not specify a particular backend namespace.","To use the OpenAI backend, you need to set your OpenAI API key and organization ID as follows:","Azure","OpenAI models can be alternatively used as a part of the Azure OpenAI service. To use the Azure backend, you need to provide your Azure API key and endpoint as follows:","When using the Azure backend, the model should be specified as . For example, if you created a gpt-3.5 deployment under the name my-model, you should use .","GPT4ALL","GPT4ALL is an open-source library that provides a unified API for multiple small-scale language models, that can be run locally on a consumer-grade hardware, even without a GPU. To use the GPT4ALL backend, you need to install the corresponding extension as follows:","Then, you can use the GPT4ALL by specifying the model as , which will be downloaded automatically. For the full list of available models, please refer to the GPT4ALL official documentation.","The models available through the GPT4ALL out of the box have very limited capabilities and are not recommended for most of the use cases. In addition, not all models are permitted for commercial use. Please check the license of the model you are using before deploying it in production.","Custom URL","Custom URL backend allows to use any GPT estimator with any OpenAI-compatible provider (either running locally or in the cloud).","In order to use the backend, it is necessary to set a global custom url:","When using  and  backends within the same script, it is necessary to reset the custom url configuration using ."]],["Vertex Family","vertex-family",["The Vertex family currently includes a single (default) backend, which is the Google Vertex AI.","In order to use the Vertex backend, you need to configure your Google Cloud credentials as follows:","Log in to Google Cloud Console and create a Google Cloud project. After the project is created, select this project from a list of projects next to the Google Cloud logo (upper left corner).","Search for Vertex AI in the search bar and select it from the list of services.","Install a Google Cloud CLI on the local machine by following the steps from the official documentation, and set the application default credentials by running the following command:","Configure Scikit-LLM with your project ID:","Additionally, for tuning LLMs in Vertex, it is required to have to have 64 cores of the TPU v3 pod training resource. By default this quota is set to 0 cores and has to be increased as follows (ignore this if you are not planning to use the tunable estimators):"]]]},{url:"/docs/quick-start",sections:[["Quick start",null,[]],["","",["To get started with Dingo, you can install the framework using pip:","Now we can create a simple pipeline that summarizes the text provided as input and translates it into French. In this particular example, we will use GPT-3.5 model from OpenAI, but Dingo supports other LLM providers as well.","Firstly, make sure to set the  environment variable to your OpenAI API key:","Next, create a new Python script and import the necessary modules:","Then, define the pipeline by creating and chaining the building blocks together:","Finally, run the pipeline with the input text:","To deploy the pipeline as a web service, you can use the following code:","This will start a web server on port 8000, exposing the pipeline as a REST API. You can now send requests using any HTTP client, or even using the official OpenAI python client library:","In this example, we have created a simple pipeline which is not designed for multi-turn conversations. To make it compatible with OpenAI Chat structure, Dingo defines special message roles like  and  which are used to pass the input arguments to the pipeline. Section Core goes into more details on differences between context and chat inputs and how to handle them in Dingo."]]]},{url:"/docs/text-summarization",sections:[["Text summarization",null,[]],["Overview","overview",["LLMs excel at performing summarization tasks. Scikit-LLM provides a summarizer that can be used both as stand-alone estimator, or as a preprocessor (in this case we can make an analogy with a dimensionality reduction preprocessor).","Example:","Please be aware that the  hyperparameter sets a soft limit, which is not strictly enforced outside of the prompt. Therefore, in some cases, the actual number of words might be slightly higher.","Additionally, it is possible to generate a summary, emphasizing a specific concept, by providing an optional parameter :"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTSummarizer"]]]},{url:"/docs/text-translation",sections:[["Text translation",null,[]],["Overview","overview",["LLMs have proven their proficiency in translation tasks. To leverage this capability, Scikit-LLM provides the Translator module, designed for translating any given text into a specified target language.","Example:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTTranslator"]]]},{url:"/docs/text-vectorization",sections:[["Text vectorization",null,[]],["Overview","overview",["LLMs can be used solely for data preprocessing by embedding a chunk of text of arbitrary length to a fixed-dimensional vector, that can be further used with virtually any model (e.g. classification, regression, clustering, etc.).","Example 1: Embedding the text","Example 2: Combining the vectorizer with the XGBoost classifier in a scikit-learn pipeline"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTVectorizer"]]]},{url:"/docs/tunable-text-classification",sections:[["Tunable text classification",null,[]],["Overview","overview",["Tunable estimators allow to fine-tune the underlying LLM for a classification task. Usually, tuning is performed directly in the cloud (e.g. OpenAI, Vertex), therefore it is not required to have a GPU on your local machine. However, be aware that tuning can be costly and time-consuming. We recommend to first try the in-context learning estimators, and only if they do not provide satisfactory results, to try the tunable estimators.","Example using GPT-3.5-Turbo-0613:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","GPTClassifier","MultiLabelGPTClassifier","VertexClassifier"]]]},{url:"/docs/tunable-text-to-text",sections:[["Tunable text-to-text",null,[]],["Overview","overview",["Tunable text-to-text estimators are estimators that can be tuned to perform a variety of tasks, including but not limited to text summarization, question answering, and text translation. These estimators use the provided data as-is, without any additional preprocessing, or constructing prompts. While this approach allows for more flexibility, it is the user's responsibility to ensure that the data is formatted correctly."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","TunableGPTText2Text","TunableVertexText2Text"]]]},{url:"/docs/use-cases-local-chatbot",sections:[["Building a local chatbot",null,[]],["Overview","overview",["In the previous tutorial, we have built a RAG chatbot using a closed-source LLM and embedding model from OpenAI. Since some users prefer running LLMs locally, this tutorial will demonstrate how to build a RAG chatbot using a fully local, open-source solution by changing just two Dingo components."]],["Chatbot Architecture and Technical Stack","chatbot-architecture-and-technical-stack",["","The application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.","FastAPI: facilitates communication between the frontend and backend.","CapybaraHermes-2.5-Mistral-7B-GGUF: LLM that generates responses upon receiving user queries.","Embedding model from SentenceTransformers: computes text embeddings.","QDrant: vector database that stores embedded chunks of text.","There are two main differences to an architecture used in the previous tutorial:","For running the model locally, Dingo can use  that is a Python binding for  library which allows to run models converted to GGUF, a binary file format for storing models for inference with .","You can find many GGUF models on Hugging Face Hub. We have chosen  model prvided by TheBloke for this tutorial.","In order to download the model, you must go to , where you will find many different files to choose from. They correspond to different quantization types of the model. Quantization involves reducing the memory needed to store model weights by decreasing their precision (for example, from 32-bit floating points to 4-bit integers). Higher precision usually leads to a higher accuracy but also requires more computational resources, which can make the model slower and more costly to operate. Decreasing the precision allows loading large models that typically would not fit into memory, and accelerating the inference. Usually, a 4-bit quantization is considered to be an optimal balance between performance, and size/speed for LLMs.","SentenceTransformers is a Python toolkit that is built on top of Hugging Face's transformers library. It facilitates using transformer models, like BERT, RoBERTa, and others, for generating sentence embeddings. These embeddings can be used for tasks such as clustering, semantic search, and classification of texts. You can check the provided pre-trained models tuned for specific tasks either on the page of SentenceTransformers here, or on the Hugging Face Hub. The models on Hugging Face Hub have a widget that allows running inferences and playing with the model directly in the browser."]],["Implementation","implementation",["As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.","The subsequent steps involve populating the vector store, creating a RAG pipeline, and building a chatbot UI. These steps are exactly the same as in the previous tutorial.","By asking a question about the Phi-3 family of models, we can verify that our local model accurately retrieves the relevant information:",""]],["Conclusion","conclusion",["In this tutorial we have built a simple local chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo provides developers with flexibility, as the components of a LLM pipeline can be easily exchanged. For example, we were able to switch from a proprietary solution to a fully open-source solution running locally by simply changing two components of the pipeline."]]]},{url:"/docs/use-cases-rag-agent",sections:[["Building a RAG agent",null,[]],["Overview","overview",["In previous tutorials, we built a pipeline that embeds the chunks of text similar to user's query to a system message, which allows the chatbot to access the external knowledge base. However, in practice, this approach may be too naive, as it:","All of these limitations can be addressed by building a more sophisticated pipeline logic, that might have a routing and query-rewriting mechanisms. However, a viable alternative is to use an  which can inherently perform all of these tasks.","The fundamental concept of agents involves using a language model to determine a sequence of actions (including the usage of external tools) and their order. One possible action could be retrieving data from an external knowledge base in response to a user's query. In this tutorial, we will develop a simple Agent that accesses multiple data sources and invokes data retrieval when needed.","As an example of external knowledge bases, we will use three webpages containing release announcement posts about recently released generative models:","Since all of these models were released recently and this information was not included in GPT-4's training data, GPT can either provide no information about these topics, or may hallucinate and generate incorrect responses (see example in my previous article here). By creating an agent that is able to retrieve data from external datasources (such as webpages linked above), we will provide an LLM with relevant contextual information that will be used for generating responses."]],["RAG Agent Architecture and Technical Stack","rag-agent-architecture-and-technical-stack",["","The application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.",": facilitates communication between the frontend and backend.",":  model from OpenAI that has access to provided knowledge bases and invokes data retrieval from them if needed.",": a vector store containing documentation about two recently released Phi-3 family of models and Llama 3.",": a vector store containing documentation about recently released OpenVoice model.","model from OpenAI: computes text embeddings.","QDrant: vector database that stores embedded chunks of text."]],["Implementation","implementation",["Indexing","Step 1:","As the first step, we need to initialize an embedding model, a chat model, and two vector stores: one for storing documentation for Llama 3 and Phi-3, and another for storing documentation for OpenVoice.","It is needed to set OPENAI_API_KEY environment variable.","Step 2:","Then, the above-mentioned websites have to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate the corresponding vector stores.","Run this script:","Step 3:","Once the vector store is created, we can create a RAG pipeline. To access the pipeline from the streamlit application, we can serve it using the  function, which provides a REST API compatible with the OpenAI API (this means that we can use an official OpenAI Python client to interact with the pipeline).","Run the script:","At this stage, we have an openai-compatible compatible backend with a model named , running on . The Streamlit application will send requests to this backend.","Step 4:","Finally, we can proceed with building a chatbot UI:","Run the application:","\uD83C\uDF89 We have successfully developed an agent that is augmented with the technical documentation of several newly released generative models, and can retrieve information from these documents if necessary. To assess the agent's ability to decide when to call the  function and its effectiveness in retrieving data from external sources, we can pose some questions about the documents provided. As you can see below, the agent generated correct responses to these questions:",""]],["Conclusion","conclusion",['In this tutorial, we have developed a RAG agent that can access external knowledge bases and retrieve data from them if needed. Unlike a "naive" RAG pipeline, the agent can selectively decide whether to access the external data, which data source to use (and how many times), and how to rewrite the user\'s query before retrieving the data. This approach allows the agent to provide more accurate and relevant responses, while the high-level pipeline logic remains as simple as of a "naive" RAG pipeline.']]]},{url:"/docs/use-cases-rag-chatbot",sections:[["Building a RAG chatbot",null,[]],["Overview","overview",["Chatbots are among the most popular use cases for large language models (LLMs). They are designed to understand and respond to user inquiries, provide answers, perform tasks, or direct users to resources. Utilizing chatbots can significantly decrease customer support costs and improve response times to user requests. However, a common issue with chatbots is their tendency to deliver generic information when users expect domain-specific responses. Additionally, they may generate outdated information when users need current updates.","For demonstrations, I have chosen the webpage about Phi-3 — a family of open AI models by Microsoft released in April 2024.","If we ask how many parameters Phi-3-mini model has, GPT-4 will generate a response indicating that it does not know the answer:","If we ask GPT-3.5 the same question, it will hallucinate and provide incorrect information:","These problems can be addressed by using the retrieval-augmented generation (RAG) technique. This technique supplements the LLM with a knowledge base external to its training data sources. For instance, an organization's internal knowledge base, such as a Wiki or internal PDFs, can be provided.","The tutorial below will demonstrate how to build a simple chatbot that utilizes RAG technique and can retrieve information about a recently released family of Phi-3 models."]],["RAG Architecture","rag-architecture",["","The basic steps of the Naive RAG include:","1. Indexing","Indexing starts with extraction of raw data from various formats such as webpage, PDF, etc. To manage the context restrictions of language models and increase the response accuracy, the extracted text is broken down into smaller, manageable chunks. For now, Dingo supports a recursive chunking that involves breaking down a large text input into smaller segments recursively until the chunks are of a desired size. The choice of the chunking size is heavily dependent on the needs of RAG application. Thus, it is recommeded to experiment with different sizes to select the best one that will allow preserving the context and maintaining the accuracy. The extracted chunks are encoded into vector representations using an embedding model and stored in a vector database.","2. Retrieval","When a user submits a query, the RAG system uses the encoding model from the indexing phase to convert the query into a vector representation. It then calculates similarity scores between the query vector and the vectors of chunks in the vector database. The system identifies and retrieves the top K chunks with the highest similarity to the query. These chunks serve as the expanded context for the prompt.","3. Generation","The users query and selected chunks are combined into a single prompt and passed to LLM. Thus, the model is provided with the necessary contextual information to formulate and deliver a response."]],["Chatbot Architecture and Technical Stack","chatbot-architecture-and-technical-stack",["","On a high level, the application will consist of the following components:","Streamlit application: provides a frontend interface for users to interact with a chatbot.",": facilitates communication between the frontend and backend.","model from OpenAI: LLM that generates responses upon receiving user queries.","model from OpenAI: computes text embeddings.","QDrant: vector database that stores embedded chunks of text."]],["Implementation","implementation",["Indexing","Step 1:","As the first step, we need to initialize an embedding model, a chat model and a vector store that will be populated with embedded chunks in the next step.","It is needed to set OPENAI_API_KEY environment variable.","Step 2:","Then, the website about Phi-3 family of models has to be parsed, chunked into smaller pieces, and embedded. The embedded chunks are used to populate a vector store.","Run this script:","At this stage, the vector store is created, allowing chunks to be retrieved and incorporated into the prompt based on a user's query.","[Optional Step]","It is also possible to identify which chunks are retrieved and check their similarity scores to the user's query:","We can see that the correct chunk was retrieved, which indeed contains information about the number of parameters in the Phi-3-mini model.","Retrieval and Augmentation","Step 3:","Once the vector store is created, we can create a RAG pipeline and serve it.","Streamlit only supports two types of messages:  and . However, it us often more appropriate to include the retrieved data into the  message. Therefore, we use a custom block that injects a  message into the chat prompt before passing it to the RAG modifier.","Run the script:","At this stage, we have a RAG pipeline compatible with the OpenAI API, named , running on . The Streamlit application will send requests to this backend.","Step 4:","Finally, we can proceed with building a chatbot UI:","Run the application:","\uD83C\uDF89 We have successfully developed a chatbot that is augmented with the technical documentation of Phi-3 family of models.If we pose the same question to this chatbot as we previously did to GPT-4 and GPT-3.5 models, we will observe that it correctly answers the question:",""]],["Conclusion","conclusion",["In this tutorial we have built a simple chatbot that utilizes RAG technique and successfully retrieves information from a vector store to generate up-to-date responses. It can be seen that Dingo enhances the development of LLM-based applications by offering essential (core) features and flexibility. That allows developers to quickly and easily create application prototypes."]]]},{url:"/docs/zero-shot-text-classification",sections:[["Zero-shot text classification",null,[]],["Overview","overview",["One of the powerful features of LLMs is the ability to perform text classification without being re-trained. For that, the only requirement is that the labels must be descriptive.","For example, let's consider a task of classifying a text into one of the following categories: [positive, negative, neutral]. We will use a class  and a regular scikit-learn API to perform the classification:","However, in the zero-shot setting, the training data is not required as it is only used for the extraction of the candidate labels. Therefore, it is sufficient to manually provide a list of candidate labels:","Additionally, it is possible to perform the classification in a multi-label setting, where multiple labels can be assigned to a single text at a same time:","Unlike in a typical supervised setting, the performance of a zero-shot classifier greatly depends on how the label itself is structured. It has to be expressed in natural language, be descriptive and self-explanatory. For example, in the previous semantic classification task, it could be beneficial to transform a label from  to ."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","ZeroShotGPTClassifier","MultiLabelZeroShotGPTClassifier","ZeroShotVertexClassifier","MultiLabelZeroShotVertexClassifier"]]]}])for(let[i,n,o]of t)X.add({url:e+(n?"#"+n:""),title:i,content:[i,...o].join("\n"),pageTitle:n?t[0][0]:void 0});function $(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},i=X.search(e,{...t,enrich:!0});return 0===i.length?[]:i[0].result.map(e=>({url:e.id,title:e.doc.title,pageTitle:e.doc.pageTitle}))}}}]);