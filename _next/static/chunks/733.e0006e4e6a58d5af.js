(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[733],{5789:function(){},5733:function(e,t,i){"use strict";var n;function s(e){return void 0===e||e}function o(e){let t=Array(e);for(let i=0;i<e;i++)t[i]=a();return t}function a(){return Object.create(null)}function r(e,t){return t.length-e.length}function l(e){return"string"==typeof e}function h(e){return"object"==typeof e}function c(e){return"function"==typeof e}function f(e,t){var i=d;if(e&&(t&&(e=p(e,t)),this.H&&(e=p(e,this.H)),this.J&&1<e.length&&(e=p(e,this.J)),i||""===i)){if(t=e.split(i),this.filter){e=this.filter,i=t.length;let n=[];for(let s=0,o=0;s<i;s++){let i=t[s];i&&!e[i]&&(n[o++]=i)}e=n}else e=t}return e}i.r(t),i.d(t,{search:function(){return $}});let d=/[\p{Z}\p{S}\p{P}\p{C}]+/u,u=/[\u0300-\u036f]/g;function m(e,t){let i=Object.keys(e),n=i.length,s=[],o="",a=0;for(let r=0,l,h;r<n;r++)(h=e[l=i[r]])?(s[a++]=g(t?"(?!\\b)"+l+"(\\b|_)":l),s[a++]=h):o+=(o?"|":"")+l;return o&&(s[a++]=g(t?"(?!\\b)("+o+")(\\b|_)":"("+o+")"),s[a]=""),s}function p(e,t){for(let i=0,n=t.length;i<n&&(e=e.replace(t[i],t[i+1]));i+=2);return e}function g(e){return RegExp(e,"g")}function y(e){let t="",i="";for(let n=0,s=e.length,o;n<s;n++)(o=e[n])!==i&&(t+=i=o);return t}function b(e){return f.call(this,(""+e).toLowerCase(),!1)}let w={},v={};function x(e){k(e,"add"),k(e,"append"),k(e,"search"),k(e,"update"),k(e,"remove")}function k(e,t){e[t+"Async"]=function(){let e;let i=this,n=arguments;var s=n[n.length-1];return c(s)&&(e=s,delete n[n.length-1]),s=new Promise(function(e){setTimeout(function(){i.async=!0;let s=i[t].apply(i,n);i.async=!1,e(s)})}),e?(s.then(e),this):s}}function T(e,t,i,n){let s=e.length,o=[],r,l,h=0;n&&(n=[]);for(let c=s-1;0<=c;c--){let f=e[c],d=f.length,u=a(),m=!r;for(let e=0;e<d;e++){let a=f[e],d=a.length;if(d)for(let e=0,f,p;e<d;e++)if(p=a[e],r){if(r[p]){if(!c){if(i)i--;else if(o[h++]=p,h===t)return o}(c||n)&&(u[p]=1),m=!0}if(n&&(f=(l[p]||0)+1,l[p]=f,f<s)){let e=n[f-2]||(n[f-2]=[]);e[e.length]=p}}else u[p]=1}if(n)r||(l=u);else if(!m)return[];r=u}if(n)for(let e=n.length-1,s,a;0<=e;e--){a=(s=n[e]).length;for(let e=0,n;e<a;e++)if(!r[n=s[e]]){if(i)i--;else if(o[h++]=n,h===t)return o;r[n]=1}}return o}function A(e){this.l=!0!==e&&e,this.cache=a(),this.h=[]}function I(e,t,i){h(e)&&(e=e.query);let n=this.cache.get(e);return n||(n=this.search(e,t,i),this.cache.set(e,n)),n}A.prototype.set=function(e,t){if(!this.cache[e]){var i=this.h.length;for(i===this.l?delete this.cache[this.h[i-1]]:i++,--i;0<i;i--)this.h[i]=this.h[i-1];this.h[0]=e}this.cache[e]=t},A.prototype.get=function(e){let t=this.cache[e];if(this.l&&t&&(e=this.h.indexOf(e))){let t=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=t}return t};let P={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function L(e,t,i,n,s,o,a,r){setTimeout(function(){let l=e(i?i+"."+n:n,JSON.stringify(a));l&&l.then?l.then(function(){t.export(e,t,i,s,o+1,r)}):t.export(e,t,i,s,o+1,r)})}function G(e,t){if(!(this instanceof G))return new G(e);if(e){l(e)?e=P[e]:(i=e.preset)&&(e=Object.assign({},i[i],e)),i=e.charset;var i,n=e.lang;l(i)&&(-1===i.indexOf(":")&&(i+=":default"),i=v[i]),l(n)&&(n=w[n])}else e={};let r,h,c=e.context||{};if(this.encode=e.encode||i&&i.encode||b,this.register=t||a(),this.D=r=e.resolution||9,this.G=t=i&&i.G||e.tokenize||"strict",this.depth="strict"===t&&c.depth,this.l=s(c.bidirectional),this.s=h=s(e.optimize),this.m=s(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=h?o(r):a(),this.A=r=c.resolution||1,this.h=h?o(r):a(),this.F=i&&i.F||e.rtl,this.H=(t=e.matcher||n&&n.H)&&m(t,!1),this.J=(t=e.stemmer||n&&n.J)&&m(t,!0),i=t=e.filter||n&&n.filter){i=t,n=a();for(let e=0,t=i.length;e<t;e++)n[i[e]]=1;i=n}this.filter=i,this.cache=(t=e.cache)&&new A(t)}function C(e,t,i,n,s){return i&&1<e?t+(n||0)<=e?i+(s||0):(e-1)/(t+(n||0))*(i+(s||0))+1|0:0}function z(e,t,i,n,s,o,r){let l=r?e.h:e.map;(!t[i]||r&&!t[i][r])&&(e.s&&(l=l[n]),r?((t=t[i]||(t[i]=a()))[r]=1,l=l[r]||(l[r]=a())):t[i]=1,l=l[i]||(l[i]=[]),e.s||(l=l[n]||(l[n]=[])),o&&l.includes(s)||(l[l.length]=s,e.m&&((e=e.register[s]||(e.register[s]=[]))[e.length]=l)))}function O(e,t,i,n,s,o,a,r){let l=[],h=r?e.h:e.map;if(e.s||(h=S(h,a,r,e.l)),h){let i=0,c=Math.min(h.length,r?e.A:e.D);for(let t=0,f=0,d,u;t<c&&(!(d=h[t])||(e.s&&(d=S(d,a,r,e.l)),s&&d&&o&&((u=d.length)<=s?(s-=u,d=null):(d=d.slice(s),s=0)),!d||(l[i++]=d,!o||!((f+=d.length)>=n))));t++);if(i)return o?F(l,n,0):void(t[t.length]=l)}return!i&&l}function F(e,t,i){return e=1===e.length?e[0]:[].concat.apply([],e),i||e.length>t?e.slice(i,i+t):e}function S(e,t,i,n){return e=i?(e=e[(n=n&&t>i)?t:i])&&e[n?i:t]:e[t]}function M(e,t,i,n,s){let o=0;if(e.constructor===Array){if(s)-1!==(t=e.indexOf(t))?1<e.length&&(e.splice(t,1),o++):o++;else{s=Math.min(e.length,i);for(let a=0,r;a<s;a++)(r=e[a])&&(o=M(r,t,i,n,s),n||o||delete e[a])}}else for(let a in e)(o=M(e[a],t,i,n,s))||delete e[a];return o}function j(e){e=e.data;var t=self._index;let i=e.args;var n=e.task;"init"===n?(n=e.options||{},e=e.factory,t=n.encode,n.cache=!1,t&&0===t.indexOf("function")&&(n.encode=Function("return "+t)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(n),delete self.FlexSearch):self._index=new G(n)):(e=e.id,t=t[n].apply(t,i),postMessage("search"===n?{id:e,msg:t}:{id:e}))}(n=G.prototype).append=function(e,t){return this.add(e,t,!0)},n.add=function(e,t,i,n){if(t&&(e||0===e)){if(!n&&!i&&this.register[e])return this.update(e,t);if(n=(t=this.encode(t)).length){let c=a(),f=a(),d=this.depth,u=this.D;for(let m=0;m<n;m++){let p=t[this.F?n-1-m:m];var s=p.length;if(p&&s>=this.B&&(d||!f[p])){var o=C(u,n,m),r="";switch(this.G){case"full":if(2<s){for(o=0;o<s;o++)for(var l=s;l>o;l--)if(l-o>=this.B){var h=C(u,n,m,s,o);z(this,f,r=p.substring(o,l),h,e,i)}break}case"reverse":if(1<s){for(l=s-1;0<l;l--)(r=p[l]+r).length>=this.B&&z(this,f,r,C(u,n,m,s,l),e,i);r=""}case"forward":if(1<s){for(l=0;l<s;l++)(r+=p[l]).length>=this.B&&z(this,f,r,o,e,i);break}default:if(this.C&&(o=Math.min(o/this.C(t,p,m)|0,u-1)),z(this,f,p,o,e,i),d&&1<n&&m<n-1){for(s=a(),r=this.A,o=p,l=Math.min(d+1,n-m),s[o]=1,h=1;h<l;h++)if((p=t[this.F?n-1-m-h:m+h])&&p.length>=this.B&&!s[p]){s[p]=1;let t=this.l&&p>o;z(this,c,t?o:p,C(r+(n/2>r?0:1),n,m,l-1,h-1),e,i,t?p:o)}}}}}this.m||(this.register[e]=1)}}return this},n.search=function(e,t,i){let n,s,o;i||(!t&&h(e)?e=(i=e).query:h(t)&&(i=t));let l=[],c,f,d=0;if(i){e=i.query||e,t=i.limit,d=i.offset||0;var u=i.context;f=i.suggest}if(e&&1<(c=(e=this.encode(""+e)).length)){i=a();var m=[];for(let t=0,n=0,s;t<c;t++)if((s=e[t])&&s.length>=this.B&&!i[s]){if(!this.s&&!f&&!this.map[s])return l;m[n++]=s,i[s]=1}c=(e=m).length}if(!c)return l;for(t||(t=100),u=this.depth&&1<c&&!1!==u,i=0,u?(n=e[0],i=1):1<c&&e.sort(r);i<c;i++){if(o=e[i],u?(s=O(this,l,f,t,d,2===c,o,n),f&&!1===s&&l.length||(n=o)):s=O(this,l,f,t,d,1===c,o),s)return s;if(f&&i===c-1){if(!(m=l.length)){if(u){u=0,i=-1;continue}return l}if(1===m)return F(l[0],t,d)}}return T(l,t,d,f)},n.contain=function(e){return!!this.register[e]},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e,t){let i=this.register[e];if(i){if(this.m)for(let t=0,n;t<i.length;t++)(n=i[t]).splice(n.indexOf(e),1);else M(this.map,e,this.D,this.s),this.depth&&M(this.h,e,this.A,this.s);if(t||delete this.register[e],this.cache){t=this.cache;for(let i=0,n;i<t.h.length;i++)n=t.h[i],t.cache[n].includes(e)&&(t.h.splice(i--,1),delete t.cache[n])}}return this},n.searchCache=I,n.export=function(e,t,i,n,s,o){let r,l,h=!0;switch(void 0===o&&(h=new Promise(e=>{o=e})),s||(s=0)){case 0:if(r="reg",this.m)for(let e in l=a(),this.register)l[e]=1;else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:void 0===i&&o&&o();return}return L(e,t||this,i,r,n,s,l,o),h},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"cfg":this.s=!!t.opt;break;case"reg":this.m=!1,this.register=t;break;case"map":this.map=t;break;case"ctx":this.h=t}},x(G.prototype);let q=0;function D(e){var t;if(!(this instanceof D))return new D(e);e?c(t=e.encode)&&(e.encode=t.toString()):e={},(t=(self||window)._factory)&&(t=t.toString());let n="undefined"==typeof window&&self.exports,s=this;this.o=function(e,t,n){let s;try{s=t?new(i(5789)).Worker("//node/node.js"):e?new Worker(URL.createObjectURL(new Blob(["onmessage="+j.toString()],{type:"text/javascript"}))):new Worker(l(n)?n:"worker/worker.js",{type:"module"})}catch(e){}return s}(t,n,e.worker),this.h=a(),this.o&&(n?this.o.on("message",function(e){s.h[e.id](e.msg),delete s.h[e.id]}):this.o.onmessage=function(e){e=e.data,s.h[e.id](e.msg),delete s.h[e.id]},this.o.postMessage({task:"init",factory:t,options:e}))}function E(e){D.prototype[e]=D.prototype[e+"Async"]=function(){let t;let i=this,n=[].slice.call(arguments);var s=n[n.length-1];return c(s)&&(t=s,n.splice(n.length-1,1)),s=new Promise(function(t){setTimeout(function(){i.h[++q]=t,i.o.postMessage({task:e,id:q,args:n})})}),t?(s.then(t),this):s}}function R(e){if(!(this instanceof R))return new R(e);var t,i=e.document||e.doc||e;this.K=[],this.h=[],this.A=[],this.register=a(),this.key=(t=i.key||i.id)&&U(t,this.A)||"id",this.m=s(e.fastupdate),this.C=(t=i.store)&&!0!==t&&[],this.store=t&&a(),this.I=(t=i.tag)&&U(t,this.A),this.l=t&&a(),this.cache=(t=e.cache)&&new A(t),e.cache=!1,this.o=e.worker,this.async=!1,t=a();let n=i.index||i.field||i;l(n)&&(n=[n]);for(let i=0,s,o;i<n.length;i++)l(s=n[i])||(o=s,s=s.field),o=h(o)?Object.assign({},e,o):e,this.o&&(t[s]=new D(o),t[s].o||(this.o=!1)),this.o||(t[s]=new G(o,this.register)),this.K[i]=U(s,this.A),this.h[i]=s;if(this.C)for(l(e=i.store)&&(e=[e]),i=0;i<e.length;i++)this.C[i]=U(e[i],this.A);this.index=t}function U(e,t){let i=e.split(":"),n=0;for(let s=0;s<i.length;s++)0<=(e=i[s]).indexOf("[]")&&(e=e.substring(0,e.length-2))&&(t[n]=!0),e&&(i[n++]=e);return n<i.length&&(i.length=n),1<n?i:i[0]}function N(e,t){if(l(t))e=e[t];else for(let i=0;e&&i<t.length;i++)e=e[t[i]];return e}function W(e,t,i,n){let s=this.l[e],o=s&&s.length-i;if(o&&0<o)return(o>t||i)&&(s=s.slice(i,i+t)),n&&(s=B.call(this,s)),{tag:e,result:s}}function B(e){let t=Array(e.length);for(let i=0,n;i<e.length;i++)n=e[i],t[i]={id:n,doc:this.store[n]};return t}E("add"),E("append"),E("search"),E("update"),E("remove"),(n=R.prototype).add=function(e,t,i){if(h(e)&&(e=N(t=e,this.key)),t&&(e||0===e)){if(!i&&this.register[e])return this.update(e,t);for(let n=0,s,o;n<this.h.length;n++)o=this.h[n],l(s=this.K[n])&&(s=[s]),function e(t,i,n,s,o,a,r,l){if(t=t[r]){if(s===i.length-1){if(t.constructor===Array){if(n[s]){for(i=0;i<t.length;i++)o.add(a,t[i],!0,!0);return}t=t.join(" ")}o.add(a,t,l,!0)}else if(t.constructor===Array)for(r=0;r<t.length;r++)e(t,i,n,s,o,a,r,l);else r=i[++s],e(t,i,n,s,o,a,r,l)}}(t,s,this.A,0,this.index[o],e,s[0],i);if(this.I){let n=N(t,this.I),s=a();l(n)&&(n=[n]);for(let t=0,o,a;t<n.length;t++)if(!s[o=n[t]]&&(s[o]=1,a=this.l[o]||(this.l[o]=[]),!i||!a.includes(e))&&(a[a.length]=e,this.m)){let t=this.register[e]||(this.register[e]=[]);t[t.length]=a}}if(this.store&&(!i||!this.store[e])){let i;if(this.C){i=a();for(let e=0,n;e<this.C.length;e++)l(n=this.C[e])?i[n]=t[n]:function e(t,i,n,s,o){if(t=t[o],s===n.length-1)i[o]=t;else if(t){if(t.constructor===Array)for(i=i[o]=Array(t.length),o=0;o<t.length;o++)e(t,i,n,s,o);else i=i[o]||(i[o]=a()),o=n[++s],e(t,i,n,s,o)}}(t,i,n,0,n[0])}this.store[e]=i||t}}return this},n.append=function(e,t){return this.add(e,t,!0)},n.update=function(e,t){return this.remove(e).add(e,t)},n.remove=function(e){if(h(e)&&(e=N(e,this.key)),this.register[e]){for(var t=0;t<this.h.length&&(this.index[this.h[t]].remove(e,!this.o),!this.m);t++);if(this.I&&!this.m)for(let i in this.l){let n=(t=this.l[i]).indexOf(e);-1!==n&&(1<t.length?t.splice(n,1):delete this.l[i])}this.store&&delete this.store[e],delete this.register[e]}return this},n.search=function(e,t,i,n){i||(!t&&h(e)?(i=e,e=""):h(t)&&(i=t,t=0));let s=[],o=[],r,c,f,d,u,m,p=0;if(i){if(i.constructor===Array)f=i,i=null;else{if(e=i.query||e,f=(r=i.pluck)||i.index||i.field,d=i.tag,c=this.store&&i.enrich,u="and"===i.bool,t=i.limit||t||100,m=i.offset||0,d&&(l(d)&&(d=[d]),!e)){for(let e=0,i;e<d.length;e++)(i=W.call(this,d[e],t,m,c))&&(s[s.length]=i,p++);return p?s:[]}l(f)&&(f=[f])}}f||(f=this.h),u=u&&(1<f.length||d&&1<d.length);let g=!n&&(this.o||this.async)&&[];for(let r=0,h,y,b;r<f.length;r++){let w;if(l(y=f[r])||(y=(w=y).field,e=w.query||e,t=w.limit||t,c=w.enrich||c),g)g[r]=this.index[y].searchAsync(e,t,w||i);else{if(b=(h=n?n[r]:this.index[y].search(e,t,w||i))&&h.length,d&&b){let e=[],i=0;u&&(e[0]=[h]);for(let t=0,n,s;t<d.length;t++)n=d[t],(b=(s=this.l[n])&&s.length)&&(i++,e[e.length]=u?[s]:s);i&&(b=(h=u?T(e,t||100,m||0):function(e,t){let i=a(),n=a(),s=[];for(let t=0;t<e.length;t++)i[e[t]]=1;for(let e=0,o;e<t.length;e++){o=t[e];for(let e=0,t;e<o.length;e++)i[t=o[e]]&&!n[t]&&(n[t]=1,s[s.length]=t)}return s}(h,e)).length)}if(b)o[p]=y,s[p++]=h;else if(u)return[]}}if(g){let n=this;return new Promise(function(s){Promise.all(g).then(function(o){s(n.search(e,t,i,o))})})}if(!p)return[];if(r&&(!c||!this.store))return s[0];for(let e=0,t;e<o.length;e++){if((t=s[e]).length&&c&&(t=B.call(this,t)),r)return t;s[e]={field:o[e],result:t}}return s},n.contain=function(e){return!!this.register[e]},n.get=function(e){return this.store[e]},n.set=function(e,t){return this.store[e]=t,this},n.searchCache=I,n.export=function(e,t,i,n,s,o){let a;if(void 0===o&&(a=new Promise(e=>{o=e})),s||(s=0),n||(n=0),n<this.h.length){let i=this.h[n],a=this.index[i];t=this,setTimeout(function(){a.export(e,t,s?i:"",n,s++,o)||(n++,s=1,t.export(e,t,i,n,s,o))})}else{let t,a;switch(s){case 1:t="tag",a=this.l,i=null;break;case 2:t="store",a=this.store,i=null;break;default:o();return}L(e,this,i,t,n,s,a,o)}return a},n.import=function(e,t){if(t)switch(l(t)&&(t=JSON.parse(t)),e){case"tag":this.l=t;break;case"reg":this.m=!1,this.register=t;for(let e=0,i;e<this.h.length;e++)(i=this.index[this.h[e]]).register=t,i.m=!1;break;case"store":this.store=t;break;default:let i=(e=e.split("."))[0];e=e[1],i&&e&&this.index[i].import(e,t)}},x(R.prototype);let _=[g("[\xe0\xe1\xe2\xe3\xe4\xe5]"),"a",g("[\xe8\xe9\xea\xeb]"),"e",g("[\xec\xed\xee\xef]"),"i",g("[\xf2\xf3\xf4\xf5\xf6ő]"),"o",g("[\xf9\xfa\xfb\xfcű]"),"u",g("[\xfdŷ\xff]"),"y",g("\xf1"),"n",g("[\xe7c]"),"k",g("\xdf"),"s",g(" & ")," and "];function H(e){var t=e=""+e;return t.normalize&&(t=t.normalize("NFD").replace(u,"")),f.call(this,t.toLowerCase(),!e.normalize&&_)}let V=/[^a-z0-9]+/,J={b:"p",v:"f",w:"f",z:"s",x:"s",ß:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Z(e){e=H.call(this,e).join(" ");let t=[];if(e){let i=e.split(V),n=i.length;for(let s=0,o,a=0;s<n;s++)if((e=i[s])&&(!this.filter||!this.filter[e])){let i=J[o=e[0]]||o,n=i;for(let t=1;t<e.length;t++){let s=J[o=e[t]]||o;s&&s!==n&&(i+=s,n=s)}t[a++]=i}}return t}let K=[g("ae"),"a",g("oe"),"o",g("sh"),"s",g("th"),"t",g("ph"),"f",g("pf"),"f",g("(?![aeo])h(?![aeo])"),"",g("(?!^[aeo])h(?!^[aeo])"),""];function Q(e,t){return e&&(2<(e=Z.call(this,e).join(" ")).length&&(e=p(e,K)),t||(1<e.length&&(e=y(e)),e&&(e=e.split(" ")))),e||[]}let X=g("(?!\\b)[aeo]");v["latin:default"]={encode:b,F:!1,G:""},v["latin:simple"]={encode:H,F:!1,G:""},v["latin:balance"]={encode:Z,F:!1,G:"strict"},v["latin:advanced"]={encode:Q,F:!1,G:""},v["latin:extra"]={encode:function(e){return e&&(1<(e=Q.call(this,e,!0)).length&&(e=e.replace(X,"")),1<e.length&&(e=y(e)),e&&(e=e.split(" "))),e||[]},F:!1,G:""};let Y=new({Index:G,Document:R,Worker:D,registerCharset:function(e,t){v[e]=t},registerLanguage:function(e,t){w[e]=t}}).Document({tokenize:"full",document:{id:"url",index:"content",store:["title","pageTitle"]},context:{resolution:9,depth:2,bidirectional:!0}});for(let{url:e,sections:t}of[{url:"/",sections:[["Quick start",null,["Scikit-LLM allows you to seamlessly integrate powerful language models into scikit-learn for enhanced text analysis tasks.","Let's see how it is possible to use Scikit-LLM to perform zero-shot text classification with GPT-4."]],["Installation","installation",["First of all, it is necessary to install Scikit-LLM. You can do it by running the following command:"]],["API Key Configuration","api-key-configuration",["For this example, we will use GPT-4, which requires an OpenAI API key. You can get one here.","Once you have your API key, you can set it as follows:","Scikit-LLM supports other language models, including the locally hosted ones. For more information, please refer to the Backend Families section."]],["Zero-Shot Text Classification","zero-shot-text-classification",["Now, we are ready to perform zero-shot text classification with GPT-4. Let's start by loading a sample dataset:","Then, we can create a classifier instance and fit it using conventional scikit-learn syntax:","Scikit-LLM will automatically query the OpenAI API and transform the response into a regular list of labels.",'Additionally, Scikit-LLM will ensure that the obtained response contains a valid label. If this is not the case, a label will be selected randomly (label probabilities are proportional to label occurrences in the "training" set).','Furthermore, since the "training" data is not strictly required, it can be fully omitted. The only thing that has to be provided is the list of candidate labels.']]]},{url:"/docs/chain-of-thought-text-classification",sections:[["Chain-of-thought text classification",null,[]],["Overview","overview",["Chain-of-thought text classification is similar to zero-shot classification since it does not require any labeled data beforehand. The only difference is that, in addition to the label itself, the model generates some additional reasoning behind its choice. In some cases, such an approach might lead to much better performance, but at the cost of higher token consumption.","Example using GPT-4o:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","CoTGPTClassifier"]]]},{url:"/docs/dynamic-few-shot-text-classification",sections:[["Dynamic few-shot text classification",null,[]],["Overview","overview",["Dynamic Few-Shot Classification is an extension of Few-Shot Text Classification that is more suitable for larger datasets. Instead of using a fixed set of examples for each class, it constructs a dynamic subset for each sample on the fly. This allows to efficiently utilize the limited contex window of the model and save the number of consumed tokens.","Let's consider a toy example, where the goal is to determine whether the review is about a book or a movie. The training dataset consists of 6 samples, 3 for each class:","Now let's say we want to classify the following review:","Since the query is about a sci-fi book, we would like to only examples 1 and 4 to be used for classification, since they are the most relevant. If we use the dynamic few-shot classifier with 1 example per class, and investigate which examples were selected, we can see that the model successfully identified examples 1 and 4 as the most relevant ones:","This is achieved by adding a KNN search algorithm as an additional preprocessor. If we assume that the most relevant examples are the closest ones in space, then the problem reduces to a nearest neighbors search and can be tackled in three steps:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","DynamicFewShotGPTClassifier"]]]},{url:"/docs/few-shot-text-classification",sections:[["Few-shot text classification",null,[]],["Overview","overview",["Few-shot text classification is a task of classifying a text into one of the pre-defined classes based on a few examples of each class. For example, given a few examples of the class positive, negative, and neutral, the model should be able to classify a new text into one of these classes.","The estimators provided by Scikit-LLM do not automatically select the subset of the training data, and instead use the entire training set to construct the examples. Therefore, if your training set is large, you might want to consider splitting it into training and validation sets, while keeping the training set small (we recommend not to exceed 10 examples per class). Additionally, it is advisable to permute the order of the samples in order to avoid the recency bias.","Example using GPT-4:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","FewShotGPTClassifier","MultiLabelFewShotGPTClassifier"]]]},{url:"/docs/how-to-contribute",sections:[["How to contribute \uD83E\uDD17",null,["Welcome! We appreciate your interest in contributing to . Whether you're a developer, designer, writer, or simply passionate about open source, there are several ways you can help improve this project. This document will guide you through the process of contributing to our Python repository."]],["How Can I Contribute?","how-can-i-contribute",["There are several ways you can contribute to this project:","Before contributing, we recommend that you open an issue to discuss your planned changes. This allows us to align our goals, provide guidance, and potentially find other contributors interested in collaborating on the same feature or bug fix.","When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license."]]]},{url:"/docs/introduction-backend-families",sections:[["Backend families",null,["On a high level, Scikit-LLM estimators are divided based on the language model backend family they use. The backend family is defined by the API format and does not necessarily correspond to the language model architecture. For example, all backends that follow the OpenAI API format are groupped into gpt family regardless the actual language model architecture or provider. Eeach backend family has its own set of estimators which are located in the  sub-module.","For example, the Zero-Shot Classifier is available as  for the gpt family, and as  for the vertex family. The separation between the backend families is necessary to allow for a reasonable level of flexibility if/when model providers introduce model-specific features that are not supported by other providers and hence cannot be easily abstracted away. At the same time, the number of model families is kept to a minimum to simplify the usage and maintenance of the library. Since the OpenAI API is by far the most popular and widely used, backends that follow that format are preferred over the others.","Whenever the backend family supports multiple backends, the default one is used unless the  parameter specifies a particular backend namespace. For example, the default backend for the gpt family is the OpenAI backend. However, you can use the Azure backend by setting . However, please note that not every estimator supports every backend."]],["GPT Family","gpt-family",["The GPT family includes all backends that follow the OpenAI API format.","OpenAI (default)","The OpenAI backend is the default backend for the GPT family. It is used whenever the  parameter does not specify a particular backend namespace.","To use the OpenAI backend, you need to set your OpenAI API key and organization ID as follows:","Azure","OpenAI models can be alternatively used as a part of the Azure OpenAI service. To use the Azure backend, you need to provide your Azure API key and endpoint as follows:","When using the Azure backend, the model should be specified as . For example, if you created a gpt-3.5 deployment under the name my-model, you should use .","GGUF","GGUF is an open-source binary file format designed for storing the quantized versions of model weights as well as high level model configurations. GGUF is primarily used in combination with the Llama CPP project, but can also be loaded by some other runtimes.","In order to use GGUF models with scikit-llm, the llama-cpp and its python bindings have to be installed first.","The installation command slightly varies depending on your hardware.","CPU-only, all platforms:","GPU (CUDA 12.1+), Windows/Linux:","GPU (Metal), MacOS, M-series Macs only:","Then, you can use the backend by specifying the model as . The model will be downloaded automatically.","Currently, the following models are available:","For all of the models, the quantized version is used. The precision is indicated by the suffix in the name (e.g.  stands for 4-bit quantization). By default, we choose the models with 4-bit quantization, but might decide to include models with lower/higher precision as well (for models with higher/lower number of parameters respectively). When picking the model for your use case, the following rule of thumb can be applied:","In addition, there exist several quantization schemas of the same precision (e.g. for q4 those can be Q4_0, Q4_K_S, Q4_K_M, etc.). In order to keep it simpler for the users, we omit this information in the model name and select a single sub-type which we consider to be the most optimal.","GPU acceleration","GGUF models can be unloaded to a GPU (both fully and partially).","The following command specifies the maximum number of GPU layers:","Note, that changing the configuration does not reload the model automatically (even if the new estimator is created afterwards). The models can be off-loaded from the memory as follows:","This command can also be handy when experimenting with different models in an interactive environment like JupyterNotebook, as the models remain in the memory until the termination of the process.","Custom URL","Custom URL backend allows to use any GPT estimator with any OpenAI-compatible provider (either running locally or in the cloud).","In order to use the backend, it is necessary to set a global custom url:","When using  and  backends within the same script, it is necessary to reset the custom url configuration using ."]],["Vertex Family","vertex-family",["The Vertex family currently includes a single (default) backend, which is the Google Vertex AI.","In order to use the Vertex backend, you need to configure your Google Cloud credentials as follows:","Log in to Google Cloud Console and create a Google Cloud project. After the project is created, select this project from a list of projects next to the Google Cloud logo (upper left corner).","Search for Vertex AI in the search bar and select it from the list of services.","Install a Google Cloud CLI on the local machine by following the steps from the official documentation, and set the application default credentials by running the following command:","Configure Scikit-LLM with your project ID:","Additionally, for tuning LLMs in Vertex, it is required to have to have 64 cores of the TPU v3 pod training resource. By default this quota is set to 0 cores and has to be increased as follows (ignore this if you are not planning to use the tunable estimators):"]],["Third party integrations","third-party-integrations",[]]]},{url:"/docs/ner",sections:[["Named Entity Recognition",null,[]],["Overview","overview",["Named Entity Recognition is an experimental feature and may be subject to instability. Please be aware that the API and/or functionality could change.","Named Entity Recognition is a process of locating and classifying the named entities in a provided text.","Currently, Scikit-LLM has a single NER estimator (only works with the GPT family) called .","Exemplary usage:","The model will tag the entities and provide a short reasoning behind its choice. If the  output is set to , the outputs of the model are parsed automatically and presented in a human readable way: each entity is highlighted and the explanation is displayed on hovering over the entity.","Exemplary output:","==============================","==============================","The  functionality works in both Jupyter Notebook and plain Python scripts. When used outside Jupyter, a new HTML page will be auto-generated and opened in a new browser window."]],["Sparse vs Dense NER","sparse-vs-dense-ner",["We distinguish between two modes of generating the predictions: sparse and dense.","In dense mode the model produces a complete (tagged) output right away, while in sparse mode only a list of entities is produced which is then mapped to the text via regex.","In most of the scenarios the usage of sparse mode should be preferable for the following reasons:","Dense mode should only be used when the following conditions are met:",'For example, in a sentence "Apple is the favorite fruit of the CEO of Apple", the first and second occurrences of the word "Apple" should be classified as different entities, which is only possible using the dense mode.']],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTExplainableNER"]]]},{url:"/docs/tagging-overview",sections:[["Overview",null,["Tagging in Scikit-LLM can be an arbitrary task that takes a raw text and returns the same text with inserted XML-like tags.","For example, a sentiment analysis task could look as follows:","Input:","Output:","In an ideal scenario, such tagging process should be invertible, so the original text can always be reconstructed from the tagged one. However, this is not always feasible and hence not considered to be a mandatory requirement."]]]},{url:"/docs/text-summarization",sections:[["Text summarization",null,[]],["Overview","overview",["LLMs excel at performing summarization tasks. Scikit-LLM provides a summarizer that can be used both as stand-alone estimator, or as a preprocessor (in this case we can make an analogy with a dimensionality reduction preprocessor).","Example:","Please be aware that the  hyperparameter sets a soft limit, which is not strictly enforced outside of the prompt. Therefore, in some cases, the actual number of words might be slightly higher.","Additionally, it is possible to generate a summary, emphasizing a specific concept, by providing an optional parameter :"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTSummarizer"]]]},{url:"/docs/text-translation",sections:[["Text translation",null,[]],["Overview","overview",["LLMs have proven their proficiency in translation tasks. To leverage this capability, Scikit-LLM provides the Translator module, designed for translating any given text into a specified target language.","Example:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTTranslator"]]]},{url:"/docs/text-vectorization",sections:[["Text vectorization",null,[]],["Overview","overview",["LLMs can be used solely for data preprocessing by embedding a chunk of text of arbitrary length to a fixed-dimensional vector, that can be further used with virtually any model (e.g. classification, regression, clustering, etc.).","Example 1: Embedding the text","Example 2: Combining the vectorizer with the XGBoost classifier in a scikit-learn pipeline"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","GPTVectorizer"]]]},{url:"/docs/tunable-text-classification",sections:[["Tunable text classification",null,[]],["Overview","overview",["Tunable estimators allow to fine-tune the underlying LLM for a classification task. Usually, tuning is performed directly in the cloud (e.g. OpenAI, Vertex), therefore it is not required to have a GPU on your local machine. However, be aware that tuning can be costly and time-consuming. We recommend to first try the in-context learning estimators, and only if they do not provide satisfactory results, to try the tunable estimators.","Example using GPT-3.5-Turbo-0613:"]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","GPTClassifier","MultiLabelGPTClassifier","VertexClassifier"]]]},{url:"/docs/tunable-text-to-text",sections:[["Tunable text-to-text",null,[]],["Overview","overview",["Tunable text-to-text estimators are estimators that can be tuned to perform a variety of tasks, including but not limited to text summarization, question answering, and text translation. These estimators use the provided data as-is, without any additional preprocessing, or constructing prompts. While this approach allows for more flexibility, it is the user's responsibility to ensure that the data is formatted correctly."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn transformer.","TunableGPTText2Text","TunableVertexText2Text"]]]},{url:"/docs/zero-shot-text-classification",sections:[["Zero-shot text classification",null,[]],["Overview","overview",["One of the powerful features of LLMs is the ability to perform text classification without being re-trained. For that, the only requirement is that the labels must be descriptive.","For example, let's consider a task of classifying a text into one of the following categories: [positive, negative, neutral]. We will use a class  and a regular scikit-learn API to perform the classification:","However, in the zero-shot setting, the training data is not required as it is only used for the extraction of the candidate labels. Therefore, it is sufficient to manually provide a list of candidate labels:","Additionally, it is possible to perform the classification in a multi-label setting, where multiple labels can be assigned to a single text at a same time:","Unlike in a typical supervised setting, the performance of a zero-shot classifier greatly depends on how the label itself is structured. It has to be expressed in natural language, be descriptive and self-explanatory. For example, in the previous semantic classification task, it could be beneficial to transform a label from  to ."]],["API Reference","api-reference",["The following API reference only lists the parameters needed for the initialization of the estimator. The remaining methods follow the syntax of a scikit-learn classifier.","ZeroShotGPTClassifier","MultiLabelZeroShotGPTClassifier","ZeroShotVertexClassifier","MultiLabelZeroShotVertexClassifier"]]]}])for(let[i,n,s]of t)Y.add({url:e+(n?"#"+n:""),title:i,content:[i,...s].join("\n"),pageTitle:n?t[0][0]:void 0});function $(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},i=Y.search(e,{...t,enrich:!0});return 0===i.length?[]:i[0].result.map(e=>({url:e.id,title:e.doc.title,pageTitle:e.doc.pageTitle}))}}}]);